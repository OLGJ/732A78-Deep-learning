{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboration\n",
    "\n",
    "Data used in this laboration are from the Kitsune Network Attack Dataset, https://archive.ics.uci.edu/ml/datasets/Kitsune+Network+Attack+Dataset . We will focus on the 'Mirai' part of the dataset. Your task is to make a DNN that can classify if each attack is benign or malicious. The dataset has 116 covariates, but to make it a bit more difficult we will remove the first 24 covariates.\n",
    "\n",
    "You need to answer all questions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Get the data\n",
    "\n",
    "Skip this part if you load stored numpy arrays (Mirai*.npy) (which is recommended)\n",
    "\n",
    "Use `wget` in the terminal of your cloud machine (in the same directory as where you have saved this notebook) to download the data, i.e.\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_dataset.csv.gz\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_labels.csv.gz\n",
    "\n",
    "Then unpack the files using `gunzip` in the terminal, i.e.\n",
    "\n",
    "gunzip Mirai_dataset.csv.gz\n",
    "\n",
    "gunzip Mirai_labels.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Get a graphics card\n",
    "\n",
    "Skip this part if you run on the CPU (recommended)\n",
    "\n",
    "Lets make sure that our script can see the graphics card that will be used. The graphics cards will perform all the time consuming calculations in every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Olof\\Downloads\\DNN_Lab_2022.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Olof/Downloads/DNN_Lab_2022.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39m# Ignore FutureWarning from numpy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Olof/Downloads/DNN_Lab_2022.ipynb#ch0000003?line=4'>5</a>\u001b[0m warnings\u001b[39m.\u001b[39msimplefilter(action\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m, category\u001b[39m=\u001b[39m\u001b[39mFutureWarning\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Olof/Downloads/DNN_Lab_2022.ipynb#ch0000003?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mK\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Olof/Downloads/DNN_Lab_2022.ipynb#ch0000003?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Olof/Downloads/DNN_Lab_2022.ipynb#ch0000003?line=9'>10</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_DEVICE_ORDER\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPCI_BUS_ID\u001b[39m\u001b[39m\"\u001b[39m;\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "# Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Hardware\n",
    "\n",
    "In deep learning, the computer hardware is very important. You should always know what kind of hardware you are working on. Lets pretend that everyone is using an Nvidia RTX 3090 graphics card.\n",
    "\n",
    "Question 1: Google the name of the graphics card, how many CUDA cores does it have?\n",
    "\n",
    "Question 2: How much memory does the graphics card have?\n",
    "\n",
    "Question 3: What is stored in the GPU memory while training a DNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: 10 496\n",
    "\n",
    "Q2: 24 GB\n",
    "\n",
    "Q3:     \n",
    "      1. Parameters — The weights and biases of the network.\n",
    "      2. Optimizer’s variables — Per-algorithm intermediate variables (e.g. momentums).\n",
    "      3. Intermediate calculations — Values from the forward pass that are temporarily stored in GPU memory and then used in the backward pass. (e.g. the activation outputs of every layer are used in the backward pass to calculate the gradients)\n",
    "      4. Workspace — Temporary memory for local variables of kernel implementations.\n",
    "\n",
    "While (1) and (4) are always required, (2) and (3) are required only in training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Load the data\n",
    "\n",
    "To make this step easier, directly load the data from saved numpy arrays (.npy) (recommended)\n",
    "\n",
    "\n",
    "Load the dataset from the csv files, it will take some time since it is almost 1.4 GB. (not recommended, unless you want to learn how to do it)\n",
    "\n",
    "We will use the function `genfromtxt` to load the data. (not recommended, unless you want to learn how to do it)\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\n",
    "\n",
    "Load the data from csv files the first time, then save the data as numpy files for faster loading the next time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariates have size (764137, 92).\n",
      "The labels have size (764137,).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 121621, 1.0: 642516}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import genfromtxt # Not needed if you load data from numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# Load data from numpy arrays, choose reduced files if the training takes too long\n",
    "X = np.load('Mirai_data.npy')\n",
    "Y = np.load('Mirai_labels.npy')\n",
    "\n",
    "\n",
    "# Remove the first 24 covariates (columns)\n",
    "X = X[:, 24:]\n",
    "\n",
    "print('The covariates have size {}.'.format(X.shape))\n",
    "print('The labels have size {}.'.format(Y.shape))\n",
    "\n",
    "# Print the number of examples of each class\n",
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: How good is a naive classifier?\n",
    "\n",
    "Question 4: Given the number of examples from each class, how high classification performance can a naive classifier obtain? The naive classifier will assume that all examples belong to one class. Note: you do not need to make a naive classifier, this is a theoretical question, just to understand how good performance we can obtain by guessing that all examples belong to one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: if p = examples of a class in a set n, then a naive classifier can achieve classification performance p/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764137\n",
      "70300604\n"
     ]
    }
   ],
   "source": [
    "# It is common to have NaNs in the data, lets check for it. Hint: np.isnan()\n",
    "\n",
    "# Print the number of NaNs (not a number) in the labels\n",
    "clean_Y = np.count_nonzero(~np.isnan(Y))\n",
    "print(clean_Y)\n",
    "\n",
    "# Print the number of NaNs in the covariates\n",
    "clean_X = np.count_nonzero(~np.isnan(X))\n",
    "print(clean_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Preprocessing\n",
    "\n",
    "Lets do some simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.309755941969869e-14\n",
      "1.0000000000003428\n"
     ]
    }
   ],
   "source": [
    "# Convert covariates to floats\n",
    "X = X.astype(float)\n",
    "\n",
    "# Convert labels to integers\n",
    "Y = Y.astype(int)\n",
    "\n",
    "# Remove mean of each covariate (column)\n",
    "X = X - np.mean(X, axis = 0)\n",
    "\n",
    "# Divide each covariate (column) by its standard deviation\n",
    "X = X/ np.std(X, axis = 0)\n",
    "\n",
    "# Check that mean is 0 and standard deviation is 1 for all covariates, by printing mean and std\n",
    "print(np.mean(X))\n",
    "print(np.std(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Split the dataset\n",
    "\n",
    "Use the first 70% of the dataset for training, leave the other 30% for validation and test, call the variables\n",
    "\n",
    "Xtrain (70%)\n",
    "\n",
    "Xtemp  (30%)\n",
    "\n",
    "Ytrain (70%)\n",
    "\n",
    "Ytemp  (30%)\n",
    "\n",
    "We use a function from scikit learn.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain has size (534895, 92).\n",
      "Ytrain has size (534895,).\n",
      "Xtemp has size (229242, 92).\n",
      "Ytemp has size (229242,).\n",
      "85248 449647\n",
      "36373 192869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code\n",
    "\n",
    "Xtrain, Xtemp, Ytrain, Ytemp = train_test_split(X, Y, train_size=0.70, random_state=42)\n",
    "\n",
    "\n",
    "#Xtrain, Xtemp = train_test_split(X, train_size = 0.70, random_state=42)\n",
    "#Ytrain, Ytemp = train_test_split(Y, train_size = 0.70, random_state=42)\n",
    "\n",
    "print('Xtrain has size {}.'.format(Xtrain.shape))\n",
    "print('Ytrain has size {}.'.format(Ytrain.shape))\n",
    "\n",
    "print('Xtemp has size {}.'.format(Xtemp.shape))\n",
    "print('Ytemp has size {}.'.format(Ytemp.shape))\n",
    "\n",
    "# Print the number of examples of each class, for the training data and the remaining 30%\n",
    "print(np.bincount(Ytrain)[0], np.bincount(Ytrain)[1])\n",
    "print(np.bincount(Ytemp)[0], np.bincount(Ytemp)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 8: Split non-training data data into validation and test\n",
    "Now split your non-training data (Xtemp, Ytemp) into 50% validation (Xval, Yval) and 50% testing (Xtest, Ytest), we use a function from scikit learn. In total this gives us 70% for training, 15% for validation, 15% for test.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Do all variables (Xtrain,Ytrain), (Xval,Yval), (Xtest,Ytest) have the shape that you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation and test data have size (114621, 92), (114621, 92), (114621,) and (114621,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "Xval, Xtest = train_test_split(Xtemp, train_size = 0.50, random_state=42)\n",
    "Yval, Ytest = train_test_split(Ytemp, train_size = 0.50, random_state=42)\n",
    "\n",
    "print('The validation and test data have size {}, {}, {} and {}'.format(Xval.shape, Xtest.shape, Yval.shape, Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: DNN classification\n",
    "\n",
    "Finish this code to create a first version of the classifier using a DNN. Start with a simple network with 2 dense layers (with 20 nodes each), using sigmoid activation functions. The final dense layer should have a single node and a sigmoid activation function. We start with the SGD optimizer.\n",
    "\n",
    "For different parts of this notebook you need to go back here, add more things, and re-run this cell to re-define the build function.\n",
    "\n",
    "Relevant functions are\n",
    "\n",
    "`model.add()`, adds a layer to the network\n",
    "\n",
    "`Dense()`, a dense network layer\n",
    "\n",
    "`model.compile()`, compile the model, add \" metrics=['accuracy'] \" to print the classification accuracy during the training\n",
    "\n",
    "See https://keras.io/layers/core/ for information on how the `Dense()` function works\n",
    "\n",
    "Import a relevant cost / loss function for binary classification from keras.losses (https://keras.io/losses/)\n",
    "\n",
    "See the following links for how to compile, train and evaluate the model\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#compile-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#fit-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#evaluate-method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Set seed from random number generator, for better comparisons\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string\n",
    "    opt = SGD(learning_rate = learning_rate)\n",
    "    \n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "    \n",
    "    \n",
    "    # Add first layer, requires input shape\n",
    "    model.add(Dense(units = n_nodes,\n",
    "                    input_dim = input_shape, \n",
    "                    activation = act_fun))\n",
    "    \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(n_layers-1):\n",
    "        model.add(Dense(units = n_nodes,\n",
    "                       activation = act_fun))\n",
    "           \n",
    "    \n",
    "    # Add final layer\n",
    "    model.add(Dense(units=1, activation = act_fun)) # 1 node\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer = opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a help function for plotting the training results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results(history):\n",
    "    \n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Train the DNN\n",
    "\n",
    "Time to train the DNN, we start simple with 2 layers with 20 nodes each, learning rate 0.1.\n",
    "\n",
    "Relevant functions\n",
    "\n",
    "`build_DNN`, the function we defined in Part 9, call it with the parameters you want to use\n",
    "\n",
    "`model.fit()`, train the model with some training data\n",
    "\n",
    "`model.evaluate()`, apply the trained model to some test data\n",
    "\n",
    "See the following links for how to train and evaluate the model\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#fit-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#evaluate-method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 1s 8ms/step - loss: 0.4178 - accuracy: 0.8406 - val_loss: 0.3955 - val_accuracy: 0.8404\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.3718 - accuracy: 0.8406 - val_loss: 0.3443 - val_accuracy: 0.8404\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8406 - val_loss: 0.2872 - val_accuracy: 0.8404\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8447 - val_loss: 0.2435 - val_accuracy: 0.8614\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.2312 - accuracy: 0.8699 - val_loss: 0.2185 - val_accuracy: 0.8797\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.2127 - accuracy: 0.8813 - val_loss: 0.2051 - val_accuracy: 0.8863\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.2025 - accuracy: 0.8926 - val_loss: 0.1973 - val_accuracy: 0.8992\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1961 - accuracy: 0.8990 - val_loss: 0.1921 - val_accuracy: 0.9020\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1917 - accuracy: 0.9015 - val_loss: 0.1883 - val_accuracy: 0.9036\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1884 - accuracy: 0.9023 - val_loss: 0.1853 - val_accuracy: 0.9044\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1856 - accuracy: 0.9033 - val_loss: 0.1827 - val_accuracy: 0.9053\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1833 - accuracy: 0.9042 - val_loss: 0.1806 - val_accuracy: 0.9063\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9050 - val_loss: 0.1787 - val_accuracy: 0.9070\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9056 - val_loss: 0.1771 - val_accuracy: 0.9077\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1780 - accuracy: 0.9060 - val_loss: 0.1756 - val_accuracy: 0.9082\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9063 - val_loss: 0.1743 - val_accuracy: 0.9084\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.9067 - val_loss: 0.1732 - val_accuracy: 0.9088\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9070 - val_loss: 0.1721 - val_accuracy: 0.9089\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9072 - val_loss: 0.1711 - val_accuracy: 0.9092\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.9074 - val_loss: 0.1703 - val_accuracy: 0.9094\n"
     ]
    }
   ],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "\n",
    "input_shape = 92\n",
    "\n",
    "# Build the model\n",
    "model1 = build_DNN(input_shape, n_layers = 2, n_nodes = 20, learning_rate = 0.1)\n",
    "\n",
    "# Train the model, provide training data and validation data\n",
    "history1 = model1.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "score = model1.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 11: More questions\n",
    "\n",
    "Question 5: What happens if you add several Dense layers without specifying the activation function?\n",
    "\n",
    "Question 6: How are the weights in each dense layer initialized as default? How are the bias weights initialized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: It will use linear activation, which in turn means adding several Dense layers is redundant.\n",
    "\n",
    "Q6: Normaly distributed with stddev = 0.01. As zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12: Balancing the classes\n",
    "\n",
    "This dataset is rather unbalanced, we need to define class weights so that the training pays more attention to the class with fewer samples. We use a function in scikit learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "\n",
    "You need to call the function something like this\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight = , classes = , y = )\n",
    "\n",
    "otherwise it will complain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.13728768 0.59479436]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(Ytrain), y = Ytrain)\n",
    "\n",
    "# Print the class weights\n",
    "print(class_weights)\n",
    "# Keras wants the weights in this form, uncomment and change value1 and value2 to your weights, \n",
    "# or get them from the array that is returned from class_weight\n",
    "\n",
    "class_weights = {0: class_weights[0],\n",
    "                 1: class_weights[1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model2 = build_DNN(input_shape, n_layers = 2, n_nodes = 20, learning_rate = 0.1)\n",
    "\n",
    "# Train the model, provide training data and validation data\n",
    "history2 = model2.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model2.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13: More questions\n",
    "\n",
    "Skip questions 8 and 9 if you run on the CPU (recommended)\n",
    "\n",
    "Question 7: Why do we have to use a batch size? Why can't we simply use all data at once? This is more relevant for even larger datasets.\n",
    "\n",
    "Question 8: How busy is the GPU for a batch size of 100? How much GPU memory is used? Hint: run 'nvidia-smi' on the computer a few times during training.\n",
    "\n",
    "Question 9: What is the processing time for one training epoch when the batch size is 100? What is the processing time for one epoch when the batch size is 1,000? What is the processing time for one epoch when the batch size is 10,000? Explain the results. \n",
    "\n",
    "Question 10: How many times are the weights in the DNN updated in each training epoch if the batch size is 100? How many times are the weights in the DNN updated in each training epoch if the batch size is 1,000? How many times are the weights in the DNN updated in each training epoch if the batch size is 10,000?  \n",
    "\n",
    "Question 11: What limits how large the batch size can be?\n",
    "\n",
    "Question 12: Generally speaking, how is the learning rate related to the batch size? If the batch size is decreased, how should the learning rate be changed?\n",
    "\n",
    "Lets use a batch size of 10,000 from now on, and a learning rate of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: We use batch sizes to split our data into more manageable sub-datasets. As we train our network on a batch instead of the entire dataset, it requires less memory which is important when we work with large dataset with a lot of features -> meaning we may not be able to train our network unless we divide it in some way.\n",
    "\n",
    "Q10: The weights are updated after processing each batch. This means that if we use a data set of 10^5 samples and a batch size of 100, we will split our data set into 1000 batches and we will updates our weights 1000 times. With a batch size of 1000 we will update the weights 100 times in each epoch. With a batch size of 100000 we will update our weights 10 times in each epoch. As such the relation is n_weight_updates = data_set_size/batch_size for each epoch.\n",
    "\n",
    "Q11: Our memory capacity of how much data we can process at once.\n",
    "\n",
    "Q12: Generally speaking the batch size and learning rate should be tuned togheter, i.e when batch size is increased learning rate should also be increased and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 14: Increasing the complexity\n",
    "\n",
    "Lets try some different configurations of number of layers and number of nodes per layer.\n",
    "\n",
    "Question 13: How many trainable parameters does the network with 4 dense layers with 50 nodes each have, compared to the initial network with 2 layers and 20 nodes per layer? Hint: use model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13: 4 layers 50 nodes have 12,351 parameters\n",
    "\n",
    "2 layers 20 nodes have 2,301 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 1s 12ms/step - loss: 0.7182 - accuracy: 0.6047 - val_loss: 0.6932 - val_accuracy: 0.4488\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.6931 - accuracy: 0.5242 - val_loss: 0.6942 - val_accuracy: 0.1568\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5127 - val_loss: 0.6940 - val_accuracy: 0.1586\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6924 - accuracy: 0.5012 - val_loss: 0.6895 - val_accuracy: 0.8398\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.7370 - val_loss: 0.6916 - val_accuracy: 0.8797\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6916 - accuracy: 0.7035 - val_loss: 0.6907 - val_accuracy: 0.8820\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6911 - accuracy: 0.7607 - val_loss: 0.6892 - val_accuracy: 0.8713\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6906 - accuracy: 0.7842 - val_loss: 0.6867 - val_accuracy: 0.8461\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6899 - accuracy: 0.7884 - val_loss: 0.6864 - val_accuracy: 0.8535\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6891 - accuracy: 0.8565 - val_loss: 0.6916 - val_accuracy: 0.8546\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.6882 - accuracy: 0.8677 - val_loss: 0.6854 - val_accuracy: 0.8910\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.6870 - accuracy: 0.8764 - val_loss: 0.6905 - val_accuracy: 0.8573\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6854 - accuracy: 0.8796 - val_loss: 0.6785 - val_accuracy: 0.8725\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6833 - accuracy: 0.8847 - val_loss: 0.6820 - val_accuracy: 0.8805\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6804 - accuracy: 0.8842 - val_loss: 0.6744 - val_accuracy: 0.8910\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.6763 - accuracy: 0.8850 - val_loss: 0.6746 - val_accuracy: 0.8820\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6702 - accuracy: 0.8831 - val_loss: 0.6630 - val_accuracy: 0.8863\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6608 - accuracy: 0.8829 - val_loss: 0.6505 - val_accuracy: 0.8853\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6452 - accuracy: 0.8833 - val_loss: 0.6307 - val_accuracy: 0.8856\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.6179 - accuracy: 0.8835 - val_loss: 0.5963 - val_accuracy: 0.8845\n"
     ]
    }
   ],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model3 = build_DNN(input_shape, n_layers = 4, n_nodes = 20, learning_rate = 0.1)\n",
    "\n",
    "history3 = model3.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)\n",
    "\n",
    "#model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model3.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model4 = build_DNN(input_shape, n_layers = 2, n_nodes = 50, learning_rate = 0.1)\n",
    "\n",
    "history4 = model4.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model4.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model5 = build_DNN(input_shape, n_layers = 4, n_nodes = 50, learning_rate = 0.1)\n",
    "\n",
    "history5 = model5.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model5.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 15: Batch normalization\n",
    "\n",
    "Now add batch normalization after each dense layer in `build_DNN`. Remember to import BatchNormalization from keras.layers. \n",
    "\n",
    "See https://keras.io/layers/normalization/ for information about how to call the function.\n",
    "\n",
    "Question 14: Why is batch normalization important when training deep networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14: When training deep networks it's important to have stable weights that doesn't become imbalanced with extreme values (or gradient that explode), as this could cause our network to only regard certain layers/activations. As such we use batch normalization to normalize the output of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Set seed from random number generator, for better comparisons\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string\n",
    "    if optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate = learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(learning_rate = learning_rate)\n",
    "        \n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "  \n",
    "\n",
    "    # Add first layer, requires input shape\n",
    "    model.add(Dense(units = n_nodes,\n",
    "                    input_dim = input_shape, \n",
    "                    activation = act_fun))\n",
    "    if use_bn:\n",
    "        model.add(BatchNormalization())\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(rate=0.5))\n",
    "    if use_custom_dropout:\n",
    "        model.add(myDropout(rate=0.5))\n",
    "        \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(n_layers-1):\n",
    "        model.add(Dense(units = n_nodes,\n",
    "                       activation = act_fun))\n",
    "        if use_bn:\n",
    "            model.add(BatchNormalization())\n",
    "        if use_dropout:\n",
    "            model.add(Dropout(rate=0.5))\n",
    "        if use_custom_dropout:\n",
    "            model.add(myDropout(rate=0.5))\n",
    "\n",
    "\n",
    "    # Add final layer\n",
    "    model.add(Dense(units=1, activation = act_fun)) # 1 node\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer = opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model6 = build_DNN(input_shape, n_layers = 2, n_nodes = 20, learning_rate = 0.1, use_bn=True)\n",
    "\n",
    "history6 = model6.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model6.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 16: Activation function\n",
    "\n",
    "Try changing the activation function in each layer from sigmoid to ReLU, write down the test accuracy.\n",
    "\n",
    "Note: the last layer should still have a sigmoid activation function.\n",
    "\n",
    "https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, ReLU, no batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model7 = build_DNN(input_shape, n_layers = 2, n_nodes = 20, learning_rate = 0.1, use_bn=False, act_fun='relu')\n",
    "\n",
    "history7 = model7.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model7.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 17: Optimizer\n",
    "\n",
    "Try changing the optimizer from SGD to Adam (with learning rate 0.1 as before). Remember to import the Adam optimizer from keras.optimizers. \n",
    "\n",
    "https://keras.io/optimizers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, Adam optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model8 = build_DNN(input_shape, n_layers = 2, n_nodes = 20, learning_rate = 0.1, \n",
    "                   use_bn=False, act_fun='sigmoid', optimizer='adam')\n",
    "\n",
    "history8 = model7.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model8.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 18: Dropout regularization\n",
    "\n",
    "Dropout is a type of regularization that can improve accuracy for validation and test data. \n",
    "\n",
    "Add a Dropout layer after each Dense layer (but not after the final dense layer) in `build_DNN`, with a dropout probability of 50%. Remember to first import the Dropout layer from keras.layers\n",
    "\n",
    "See https://keras.io/api/layers/regularization_layers/dropout/ for how the Dropout layer works.\n",
    "\n",
    "---\n",
    "\n",
    "Question 15: How does the validation accuracy change when adding dropout?\n",
    "\n",
    "Question 16: How does the test accuracy change when adding dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15:  With dropout val_accuracy: 0.8840.\n",
    "        W/o dropout: val_accuracy: 0.9042. It decreases.\n",
    "        \n",
    "        \n",
    "Q16: With dropout test accuracy: 0.8819\n",
    "        W/o dropout: 0.9022. It decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, dropout, SGD optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.6927 - accuracy: 0.5835 - val_loss: 0.6127 - val_accuracy: 0.8822\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 1s 12ms/step - loss: 0.6111 - accuracy: 0.6684 - val_loss: 0.5269 - val_accuracy: 0.8821\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.5372 - accuracy: 0.7462 - val_loss: 0.4313 - val_accuracy: 0.8821\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4671 - accuracy: 0.7953 - val_loss: 0.3586 - val_accuracy: 0.8820\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.4148 - accuracy: 0.8236 - val_loss: 0.3201 - val_accuracy: 0.8820\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.3798 - accuracy: 0.8398 - val_loss: 0.3012 - val_accuracy: 0.8819\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3546 - accuracy: 0.8507 - val_loss: 0.2903 - val_accuracy: 0.8820\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.3393 - accuracy: 0.8575 - val_loss: 0.2865 - val_accuracy: 0.8820\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.3261 - accuracy: 0.8619 - val_loss: 0.2827 - val_accuracy: 0.8820\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.3181 - accuracy: 0.8648 - val_loss: 0.2809 - val_accuracy: 0.8820\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.3109 - accuracy: 0.8668 - val_loss: 0.2791 - val_accuracy: 0.8820\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.3038 - accuracy: 0.8685 - val_loss: 0.2782 - val_accuracy: 0.8821\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.2976 - accuracy: 0.8704 - val_loss: 0.2759 - val_accuracy: 0.8821\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.2930 - accuracy: 0.8714 - val_loss: 0.2755 - val_accuracy: 0.8821\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.2891 - accuracy: 0.8726 - val_loss: 0.2747 - val_accuracy: 0.8820\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.2852 - accuracy: 0.8729 - val_loss: 0.2727 - val_accuracy: 0.8821\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.2807 - accuracy: 0.8741 - val_loss: 0.2709 - val_accuracy: 0.8823\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.2775 - accuracy: 0.8749 - val_loss: 0.2696 - val_accuracy: 0.8826\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.2738 - accuracy: 0.8756 - val_loss: 0.2678 - val_accuracy: 0.8832\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.2716 - accuracy: 0.8759 - val_loss: 0.2671 - val_accuracy: 0.8837\n"
     ]
    }
   ],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train model\n",
    "model9 = build_DNN(input_shape, n_layers = 2, n_nodes = 20, learning_rate = 0.1, \n",
    "                   use_dropout = True, act_fun='sigmoid', optimizer='sgd')\n",
    "\n",
    "history9 = model9.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model9.evaluate(Xtest, Ytest)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 19: Improving performance\n",
    "\n",
    "Spend some time (30 - 90 minutes) playing with the network architecture (number of layers, number of nodes per layer, activation function) and other hyper parameters (optimizer, learning rate, batch size, number of epochs, degree of regularization). For example, try a much deeper network. How much does the training time increase for a network with 10 layers?\n",
    "\n",
    "Question 17: How high classification accuracy can you achieve for the test data? What is your best configuration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeper the network, the more time it takes to train the network. \n",
    "After playing around with the hyperparameters the settings below resulted in the highest classification accuracy for the test data. \n",
    "\n",
    "Q17: 89.72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.7146 - accuracy: 0.5225 - val_loss: 0.6965 - val_accuracy: 0.1596\n",
      "Epoch 2/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.6846 - accuracy: 0.5546 - val_loss: 0.6665 - val_accuracy: 0.8885\n",
      "Epoch 3/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.6591 - accuracy: 0.6109 - val_loss: 0.5984 - val_accuracy: 0.8863\n",
      "Epoch 4/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.5608 - accuracy: 0.7214 - val_loss: 0.3878 - val_accuracy: 0.8824\n",
      "Epoch 5/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.3996 - accuracy: 0.8289 - val_loss: 0.2968 - val_accuracy: 0.8818\n",
      "Epoch 6/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.3196 - accuracy: 0.8650 - val_loss: 0.2958 - val_accuracy: 0.8818\n",
      "Epoch 7/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2917 - accuracy: 0.8739 - val_loss: 0.2919 - val_accuracy: 0.8818\n",
      "Epoch 8/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2724 - accuracy: 0.8778 - val_loss: 0.2862 - val_accuracy: 0.8819\n",
      "Epoch 9/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2600 - accuracy: 0.8802 - val_loss: 0.2786 - val_accuracy: 0.8832\n",
      "Epoch 10/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2514 - accuracy: 0.8818 - val_loss: 0.2799 - val_accuracy: 0.8843\n",
      "Epoch 11/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2444 - accuracy: 0.8834 - val_loss: 0.2713 - val_accuracy: 0.8874\n",
      "Epoch 12/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2380 - accuracy: 0.8847 - val_loss: 0.2686 - val_accuracy: 0.8902\n",
      "Epoch 13/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2329 - accuracy: 0.8862 - val_loss: 0.2674 - val_accuracy: 0.8930\n",
      "Epoch 14/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2282 - accuracy: 0.8878 - val_loss: 0.2627 - val_accuracy: 0.8950\n",
      "Epoch 15/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.2248 - accuracy: 0.8889 - val_loss: 0.2595 - val_accuracy: 0.8962\n",
      "Epoch 16/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.2217 - accuracy: 0.8901 - val_loss: 0.2568 - val_accuracy: 0.8969\n",
      "Epoch 17/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2183 - accuracy: 0.8912 - val_loss: 0.2565 - val_accuracy: 0.8974\n",
      "Epoch 18/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.2157 - accuracy: 0.8920 - val_loss: 0.2552 - val_accuracy: 0.8976\n",
      "Epoch 19/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2136 - accuracy: 0.8930 - val_loss: 0.2542 - val_accuracy: 0.8979\n",
      "Epoch 20/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2116 - accuracy: 0.8938 - val_loss: 0.2514 - val_accuracy: 0.8981\n",
      "Epoch 21/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.2095 - accuracy: 0.8948 - val_loss: 0.2527 - val_accuracy: 0.8984\n",
      "Epoch 22/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2075 - accuracy: 0.8953 - val_loss: 0.2484 - val_accuracy: 0.8987\n",
      "Epoch 23/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2062 - accuracy: 0.8959 - val_loss: 0.2482 - val_accuracy: 0.8990\n",
      "Epoch 24/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2047 - accuracy: 0.8963 - val_loss: 0.2457 - val_accuracy: 0.8996\n",
      "Epoch 25/25\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.2035 - accuracy: 0.8970 - val_loss: 0.2457 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Find your best configuration for the DNN\n",
    "batch_size = 10000\n",
    "epochs = 25\n",
    "input_shape = 92\n",
    "\n",
    "# Build and train DNN\n",
    "model10 = build_DNN(input_shape, n_layers = 3, n_nodes = 50, learning_rate = 0.3, \n",
    "                   use_dropout = True, act_fun='sigmoid', optimizer='sgd')\n",
    "\n",
    "history10 = model10.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DNN on test data\n",
    "score = model10.evaluate(Xtest, Ytest)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 20: Dropout uncertainty\n",
    "\n",
    "Dropout can also be used during testing, to obtain an estimate of the model uncertainty. Since dropout will randomly remove connections, the network will produce different results every time the same (test) data is put into the network. This technique is called Monte Carlo dropout. For more information, see this paper http://proceedings.mlr.press/v48/gal16.pdf\n",
    "\n",
    "To achieve this, we need to redefine the Keras Dropout call by running the cell below, and use 'myDropout' in each call to Dropout, in the cell that defines the DNN. The `build_DNN` function takes two boolean arguments, use_dropout and use_custom_dropout, add a standard Dropout layer if use_dropout is true, add a myDropout layer if use_custom_dropout is true.\n",
    "\n",
    "Run the same test data through the trained network 100 times, with dropout turned on. \n",
    "\n",
    "Question 18: What is the mean and the standard deviation of the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "class myDropout(keras.layers.Dropout):\n",
    "    \"\"\"Applies Dropout to the input.\n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n",
    "           http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, training=True, noise_shape=None, seed=None, **kwargs):\n",
    "        super(myDropout, self).__init__(rate, noise_shape=None, seed=None,**kwargs)\n",
    "        self.training = training\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed)\n",
    "            if not training: \n",
    "                return K.in_train_phase(dropped_inputs, inputs, training=self.training)\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "54/54 [==============================] - 2s 21ms/step - loss: 0.7080 - accuracy: 0.5345 - val_loss: 0.6894 - val_accuracy: 0.5394\n",
      "Epoch 2/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.6667 - accuracy: 0.5941 - val_loss: 0.6636 - val_accuracy: 0.5911\n",
      "Epoch 3/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.5849 - accuracy: 0.6917 - val_loss: 0.5059 - val_accuracy: 0.7610\n",
      "Epoch 4/25\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.4194 - accuracy: 0.8072 - val_loss: 0.3816 - val_accuracy: 0.8422\n",
      "Epoch 5/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.3223 - accuracy: 0.8564 - val_loss: 0.3461 - val_accuracy: 0.8656\n",
      "Epoch 6/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2862 - accuracy: 0.8699 - val_loss: 0.3148 - val_accuracy: 0.8754\n",
      "Epoch 7/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2678 - accuracy: 0.8752 - val_loss: 0.3030 - val_accuracy: 0.8789\n",
      "Epoch 8/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2555 - accuracy: 0.8781 - val_loss: 0.2964 - val_accuracy: 0.8806\n",
      "Epoch 9/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2476 - accuracy: 0.8791 - val_loss: 0.2909 - val_accuracy: 0.8818\n",
      "Epoch 10/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2410 - accuracy: 0.8806 - val_loss: 0.2802 - val_accuracy: 0.8836\n",
      "Epoch 11/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2353 - accuracy: 0.8819 - val_loss: 0.2809 - val_accuracy: 0.8844\n",
      "Epoch 12/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2313 - accuracy: 0.8830 - val_loss: 0.2727 - val_accuracy: 0.8859\n",
      "Epoch 13/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2271 - accuracy: 0.8844 - val_loss: 0.2718 - val_accuracy: 0.8868\n",
      "Epoch 14/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2248 - accuracy: 0.8852 - val_loss: 0.2649 - val_accuracy: 0.8878\n",
      "Epoch 15/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2215 - accuracy: 0.8865 - val_loss: 0.2669 - val_accuracy: 0.8888\n",
      "Epoch 16/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2186 - accuracy: 0.8878 - val_loss: 0.2612 - val_accuracy: 0.8899\n",
      "Epoch 17/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2160 - accuracy: 0.8888 - val_loss: 0.2593 - val_accuracy: 0.8912\n",
      "Epoch 18/25\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.2142 - accuracy: 0.8896 - val_loss: 0.2577 - val_accuracy: 0.8923\n",
      "Epoch 19/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2116 - accuracy: 0.8908 - val_loss: 0.2536 - val_accuracy: 0.8934\n",
      "Epoch 20/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2102 - accuracy: 0.8918 - val_loss: 0.2531 - val_accuracy: 0.8941\n",
      "Epoch 21/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2082 - accuracy: 0.8926 - val_loss: 0.2509 - val_accuracy: 0.8953\n",
      "Epoch 22/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2067 - accuracy: 0.8935 - val_loss: 0.2493 - val_accuracy: 0.8958\n",
      "Epoch 23/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2052 - accuracy: 0.8943 - val_loss: 0.2487 - val_accuracy: 0.8963\n",
      "Epoch 24/25\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2040 - accuracy: 0.8949 - val_loss: 0.2446 - val_accuracy: 0.8969\n",
      "Epoch 25/25\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.2030 - accuracy: 0.8956 - val_loss: 0.2446 - val_accuracy: 0.8981\n"
     ]
    }
   ],
   "source": [
    "# Your best training parameters\n",
    "batch_size = 10000\n",
    "epochs = 25\n",
    "input_shape = 92\n",
    "\n",
    "\n",
    "# Build and train model\n",
    "model11 = build_DNN(input_shape, n_layers = 3, n_nodes = 50, learning_rate = 0.3, \n",
    "                   use_custom_dropout = True, act_fun='sigmoid', optimizer='sgd')\n",
    "\n",
    "history11 = model11.fit(Xtrain, Ytrain, validation_data = (Xval, Yval), epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell a few times to evalute the model on test data, \n",
    "# if you get slightly different test accuracy every time, Dropout during testing is working\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model11.evaluate(Xtest, Ytest)\n",
    "                       \n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "for _ in range(0, 20):\n",
    "    score = model11.evaluate(Xtest, Ytest)\n",
    "    print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testing 100 times, and save the accuracies in an array\n",
    "accuracies = np.empty(shape=100)\n",
    "for i in range(0, 100):\n",
    "    score = model11.evaluate(Xtest, Ytest)\n",
    "    accuracies[i] = score[1]\n",
    "    \n",
    "# Calculate and print mean and std of accuracies\n",
    "print(np.mean(accuracies))\n",
    "print(np.std(accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 21: Cross validation uncertainty\n",
    "\n",
    "Cross validation (CV) is often used to evaluate a model, by training and testing using different subsets of the data it is possible to get the uncertainty as the standard deviation over folds. We here use a help function from scikit-learn to setup the CV, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html . Use 10 folds with shuffling, random state 1234. \n",
    "\n",
    "Note: We here assume that you have found the best hyper parameters, so here the data are only split into training and testing, no validation.\n",
    "\n",
    "---\n",
    "\n",
    "Question 19: What is the mean and the standard deviation of the test accuracy?\n",
    "\n",
    "Question 20: What is the main advantage of dropout compared to CV for estimating test uncertainty? The difference may not be so large in this notebook, but imagine that you have a network that takes 24 hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.6403 - accuracy: 0.8607\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3137 - accuracy: 0.8862\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2109 - accuracy: 0.8919\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1989 - accuracy: 0.8965\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1925 - accuracy: 0.8979\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1880 - accuracy: 0.8993\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1846 - accuracy: 0.9016\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1819 - accuracy: 0.9046\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1797 - accuracy: 0.9070\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1779 - accuracy: 0.9085\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1765 - accuracy: 0.9094\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1753 - accuracy: 0.9100\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1743 - accuracy: 0.9106\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1734 - accuracy: 0.9117\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1725 - accuracy: 0.9127\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1718 - accuracy: 0.9132\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1711 - accuracy: 0.9136\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1704 - accuracy: 0.9139\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1698 - accuracy: 0.9141\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1693 - accuracy: 0.9143\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1687 - accuracy: 0.9145\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1683 - accuracy: 0.9147\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1678 - accuracy: 0.9148\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1673 - accuracy: 0.9149\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1669 - accuracy: 0.9150\n",
      "2388/2388 [==============================] - 2s 754us/step - loss: 0.2107 - accuracy: 0.9144\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.6674 - accuracy: 0.7526\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3871 - accuracy: 0.8839\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2188 - accuracy: 0.8904\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2030 - accuracy: 0.8956\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1956 - accuracy: 0.8977\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1905 - accuracy: 0.8998\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1868 - accuracy: 0.9019\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1839 - accuracy: 0.9043\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1817 - accuracy: 0.9070\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1797 - accuracy: 0.9091\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1780 - accuracy: 0.9099\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1766 - accuracy: 0.9104\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1753 - accuracy: 0.9107\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1742 - accuracy: 0.9109\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1733 - accuracy: 0.9111\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1725 - accuracy: 0.9112\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1717 - accuracy: 0.9114\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1710 - accuracy: 0.9120\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1704 - accuracy: 0.9129\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1698 - accuracy: 0.9133\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1691 - accuracy: 0.9135\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1686 - accuracy: 0.9139\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1680 - accuracy: 0.9142\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1675 - accuracy: 0.9145\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1671 - accuracy: 0.9147\n",
      "2388/2388 [==============================] - 2s 766us/step - loss: 0.2057 - accuracy: 0.9170\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.6365 - accuracy: 0.7895\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2971 - accuracy: 0.8846\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2104 - accuracy: 0.8905\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1982 - accuracy: 0.8957\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1916 - accuracy: 0.8976\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1872 - accuracy: 0.9002\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1840 - accuracy: 0.9043\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1815 - accuracy: 0.9071\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1797 - accuracy: 0.9087\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1782 - accuracy: 0.9096\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1770 - accuracy: 0.9102\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1759 - accuracy: 0.9106\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1750 - accuracy: 0.9108\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1741 - accuracy: 0.9114\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1733 - accuracy: 0.9123\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1726 - accuracy: 0.9128\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1720 - accuracy: 0.9130\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1713 - accuracy: 0.9131\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1708 - accuracy: 0.9133\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1702 - accuracy: 0.9134\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1697 - accuracy: 0.9136\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1692 - accuracy: 0.9137\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1688 - accuracy: 0.9138\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1683 - accuracy: 0.9139\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1679 - accuracy: 0.9141\n",
      "2388/2388 [==============================] - 2s 903us/step - loss: 0.2097 - accuracy: 0.9145\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.6481 - accuracy: 0.8172\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.3331 - accuracy: 0.8805\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.2159 - accuracy: 0.8856\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.2023 - accuracy: 0.8939\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1947 - accuracy: 0.8972\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1892 - accuracy: 0.8989\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1851 - accuracy: 0.9018\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1820 - accuracy: 0.9052\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1797 - accuracy: 0.9078\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1778 - accuracy: 0.9095\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1763 - accuracy: 0.9101\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1751 - accuracy: 0.9106\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1740 - accuracy: 0.9109\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1731 - accuracy: 0.9112\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1723 - accuracy: 0.9125\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1715 - accuracy: 0.9133\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1708 - accuracy: 0.9135\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1701 - accuracy: 0.9137\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1695 - accuracy: 0.9139\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1689 - accuracy: 0.9142\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1683 - accuracy: 0.9145\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1678 - accuracy: 0.9147\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1674 - accuracy: 0.9149\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1669 - accuracy: 0.9150\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1665 - accuracy: 0.9152\n",
      "2388/2388 [==============================] - 2s 742us/step - loss: 0.2181 - accuracy: 0.9134\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.6609 - accuracy: 0.8020\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3711 - accuracy: 0.8804\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.2175 - accuracy: 0.8841\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.2026 - accuracy: 0.8939\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1951 - accuracy: 0.8976\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1900 - accuracy: 0.8992\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1861 - accuracy: 0.9014\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1832 - accuracy: 0.9043\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1807 - accuracy: 0.9069\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1787 - accuracy: 0.9087\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1771 - accuracy: 0.9098\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1757 - accuracy: 0.9104\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1746 - accuracy: 0.9107\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1736 - accuracy: 0.9110\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1727 - accuracy: 0.9112\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1718 - accuracy: 0.9119\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1710 - accuracy: 0.9130\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1702 - accuracy: 0.9133\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1695 - accuracy: 0.9134\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1689 - accuracy: 0.9135\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1683 - accuracy: 0.9138\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1677 - accuracy: 0.9143\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1672 - accuracy: 0.9147\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1667 - accuracy: 0.9150\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1662 - accuracy: 0.9152\n",
      "2388/2388 [==============================] - 2s 736us/step - loss: 0.2130 - accuracy: 0.9145\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.6627 - accuracy: 0.7472\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3692 - accuracy: 0.8799\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2195 - accuracy: 0.8851\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2035 - accuracy: 0.8935\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1955 - accuracy: 0.8975\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1902 - accuracy: 0.8996\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1864 - accuracy: 0.9020\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1836 - accuracy: 0.9044\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1814 - accuracy: 0.9074\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1795 - accuracy: 0.9091\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1779 - accuracy: 0.9100\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1766 - accuracy: 0.9106\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1755 - accuracy: 0.9108\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1745 - accuracy: 0.9110\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1736 - accuracy: 0.9112\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1728 - accuracy: 0.9113\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1721 - accuracy: 0.9115\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1714 - accuracy: 0.9122\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1708 - accuracy: 0.9130\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1702 - accuracy: 0.9136\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1696 - accuracy: 0.9140\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1691 - accuracy: 0.9142\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1686 - accuracy: 0.9145\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1682 - accuracy: 0.9146\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1677 - accuracy: 0.9148\n",
      "2388/2388 [==============================] - 2s 873us/step - loss: 0.2105 - accuracy: 0.9146\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.6489 - accuracy: 0.8115\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.3255 - accuracy: 0.8828\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.2147 - accuracy: 0.8880\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.2020 - accuracy: 0.8938\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1948 - accuracy: 0.8972\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1895 - accuracy: 0.8990\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1857 - accuracy: 0.9015\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1828 - accuracy: 0.9044\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1805 - accuracy: 0.9072\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1785 - accuracy: 0.9090\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1768 - accuracy: 0.9099\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1755 - accuracy: 0.9104\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1744 - accuracy: 0.9107\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1734 - accuracy: 0.9109\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1725 - accuracy: 0.9111\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1718 - accuracy: 0.9121\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1711 - accuracy: 0.9133\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1704 - accuracy: 0.9138\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1698 - accuracy: 0.9141\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1693 - accuracy: 0.9143\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1687 - accuracy: 0.9145\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1683 - accuracy: 0.9147\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1678 - accuracy: 0.9148\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1674 - accuracy: 0.9149\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1670 - accuracy: 0.9150\n",
      "2388/2388 [==============================] - 2s 838us/step - loss: 0.2110 - accuracy: 0.9169\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.6661 - accuracy: 0.7516\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.3757 - accuracy: 0.8825\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.2152 - accuracy: 0.8894\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.2006 - accuracy: 0.8956\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1931 - accuracy: 0.8978\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1879 - accuracy: 0.9006\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1841 - accuracy: 0.9034\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1812 - accuracy: 0.9063\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1789 - accuracy: 0.9090\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1771 - accuracy: 0.9101\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1757 - accuracy: 0.9105\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1746 - accuracy: 0.9109\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1737 - accuracy: 0.9111\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1728 - accuracy: 0.9113\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1721 - accuracy: 0.9114\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1715 - accuracy: 0.9120\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1709 - accuracy: 0.9131\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1703 - accuracy: 0.9135\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1698 - accuracy: 0.9136\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1693 - accuracy: 0.9137\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1688 - accuracy: 0.9138\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1684 - accuracy: 0.9139\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1679 - accuracy: 0.9140\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1675 - accuracy: 0.9142\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1671 - accuracy: 0.9145\n",
      "2388/2388 [==============================] - 2s 862us/step - loss: 0.2132 - accuracy: 0.9145\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.6334 - accuracy: 0.8525\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.3017 - accuracy: 0.8842\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.2101 - accuracy: 0.8918\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1981 - accuracy: 0.8966\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1916 - accuracy: 0.8982\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1870 - accuracy: 0.9001\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1836 - accuracy: 0.9034\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1809 - accuracy: 0.9061\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1788 - accuracy: 0.9084\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1770 - accuracy: 0.9095\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1756 - accuracy: 0.9102\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1744 - accuracy: 0.9107\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1733 - accuracy: 0.9110\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1724 - accuracy: 0.9119\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1716 - accuracy: 0.9130\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1708 - accuracy: 0.9133\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1701 - accuracy: 0.9134\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1695 - accuracy: 0.9136\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1689 - accuracy: 0.9138\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1684 - accuracy: 0.9140\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1679 - accuracy: 0.9143\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1674 - accuracy: 0.9145\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1669 - accuracy: 0.9148\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1665 - accuracy: 0.9149\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1660 - accuracy: 0.9151\n",
      "2388/2388 [==============================] - 2s 740us/step - loss: 0.2170 - accuracy: 0.9148\n",
      "Epoch 1/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.6611 - accuracy: 0.7559\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3527 - accuracy: 0.8800\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.2197 - accuracy: 0.8805\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.2058 - accuracy: 0.8868\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1977 - accuracy: 0.8961\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1916 - accuracy: 0.8982\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1870 - accuracy: 0.9004\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1834 - accuracy: 0.9033\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1808 - accuracy: 0.9058\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1788 - accuracy: 0.9087\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1772 - accuracy: 0.9100\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1758 - accuracy: 0.9105\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1748 - accuracy: 0.9109\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1738 - accuracy: 0.9110\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1730 - accuracy: 0.9112\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1723 - accuracy: 0.9113\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1716 - accuracy: 0.9114\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1710 - accuracy: 0.9115\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1704 - accuracy: 0.9119\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1699 - accuracy: 0.9128\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.1694 - accuracy: 0.9134\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1689 - accuracy: 0.9135\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1684 - accuracy: 0.9136\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.1680 - accuracy: 0.9138\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.1675 - accuracy: 0.9139\n",
      "2388/2388 [==============================] - 2s 725us/step - loss: 0.2101 - accuracy: 0.9136\n",
      "0.914835947751999\n",
      "0.0011475940565350649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10, random_state=1234, shuffle=True)\n",
    "\n",
    "# np array for keeping score\n",
    "np_score = np.empty(shape=10)\n",
    "# Loop over cross validation folds\n",
    "ctr = 0\n",
    "for train_split, test_split in skf.split(X, Y):    \n",
    "    # Calculate class weights for current split\n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight(class_weight = 'balanced', \n",
    "                                                      classes = np.unique(Y[train_split]), y = Y[train_split])\n",
    "    \n",
    "    class_weights = {0: class_weights[0],\n",
    "                 1: class_weights[1]}\n",
    "    \n",
    "    # Rebuild the DNN model, to not continue training on the previously trained model\n",
    "    rebuild_model = build_DNN(input_shape, n_layers = 3, n_nodes = 50, learning_rate = 0.3, \n",
    "                   use_dropout = False, act_fun='sigmoid', optimizer='sgd')\n",
    "    \n",
    "    # Fit the model with training set and class weights for this fold\n",
    "    fit_model = rebuild_model.fit(X[train_split], Y[train_split], validation_data = False, epochs=epochs, batch_size = batch_size,\n",
    "                     class_weight = class_weights)\n",
    "    \n",
    "    # Evaluate the model using the test set for this fold\n",
    "    score = rebuild_model.evaluate(X[test_split], Y[test_split])\n",
    "    \n",
    "    # Save the test accuracy in an array\n",
    "\n",
    "    np_score[ctr]= score[1]\n",
    "    \n",
    "    ctr += 1\n",
    "    \n",
    "# Calculate and print mean and std of accuracies\n",
    "print(np.mean(np_score))\n",
    "print(np.std(np_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q20: Running dropout requires us to run the model with dropout once, but using CV we will run the model k-folds time which in some instances can be cumbersome/take too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 22: DNN regression\n",
    "\n",
    "A similar DNN can be used for regression, instead of classification.\n",
    "\n",
    "Question 21: How would you change the DNN in order to use it for regression instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q21: \n",
    "We would change to one output neuron/node and change the loss function to something like mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Send in this jupyter notebook, with answers to all questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
